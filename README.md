
# Table of Contents

- [Repository structure](#repository-structure)
- [Portability & reproducibility](#portability--reproducibility)
  - [Code management](#code-management)
  - [Package management](#package-management)
  - [Data management](#data-management)
  - [Analysis output management](#analysis-output-management)
- [Setup steps (with an example)](#setup-steps-with-an-example)


# Repository structure
**Main files/folders**
- `/0_raw/`: Contains raw data, images, etc that are inputs to the code downstream. Files here are static and should not be affected by any code.
  - `[subfolder]/`: Contains the raw files, any data dictionaries, a `README.md` describing the provenance 
- `/1_data_prep/`: Contains code for pulling, cleaning, wrangling, etc. the data
  - `[submodule 1]/`
      - `output/`
      - `build_data.py`    
  - `run_all.sh`: Single script to run all the data prep code (especially if order matters)
- `/2_analysis/`: Contains code for running analyses
  - `[submodule 1]/`
      - `output/`
      - `summary_stats.py`
  - `run_all.sh`: Single script for running all analyses code (especially if order matters)
- `/3_sandbox/`: Contains subfolders that are for testing out code, exploring, etc.
- `/4_notes`: Latex notes (each in separate folder) that use inputs generated by code
- `/5_slides/`: Makes the slides
- `/6_paper/`: Makes the paper drafts
- `/lib`: Contains code libraries or scripts that are frequently used (e.g., the `run_latex.sh` command)
- `config.yaml`: Single file containing key parameters that are used in many places (e.g., BigQuery table locations)

**Organizing**

- Within a module (e.g., `1_data_prep`), files should be further organized into `submodules` that roughly correspond to a "task." For example, for some traffic analyses we might make a set of folders `0_raw/traffic`, `1_data_prep/traffic/`, `1_data_prep/traffic/`, etc.
    - Save cleaned data to `1_data_prep/[submodule]/output` and cleaned analyses to `2_analysis/[submodule]/output`. 
- Keep `run_all.sh` up to date -- while it is rare we will want to run everything in a folder start to finish, it helps serve as documentation for the order code should be run in. 
- Code should _not_ directly save to `input/` in of the slides, paper, or notes folders, and `.tex` files should _not_ directly reference files in any `output/` folder. Instead, copy the needed output over to an `input/` folder within the corresponding document's folder. Ideally, the copying step should be part of the `make.sh` file in the document's folder.

# Portability & reproducibility

A key goal is to ensure that all analyses are **portable** and **reproducible**, meaning they can be run from any computer and generate the same results. This comes with three key requirements: 1) everyone can run the same code from anywhere; 3) environments, including R/Python package versions, are the same everywhere; and 3) data files are the same everywhere. 

## Code management 
We use Github to manage code. Some best practices: 
- Work within [branches](https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/proposing-changes-to-your-work-with-pull-requests/about-branches) and submit [pull requests](https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/proposing-changes-to-your-work-with-pull-requests/about-pull-requests) when the changes are ready to be integrated into the main repository. Do not commit changes directly to the main repository. 
- Data files should not be committed; see below.
- Use the `.gitignore` to automatically exclude certain files or file types from being committed. 
- Use issues on Github to discuss and review code. You can link commits to issues by including `#[issue number]` in the commit message. 


## Package management

### Python
We use `uv` to manage Python environments and packages. For those familiar with Python, `uv` is a modern version of `pip` that solves many of the pains people run into with dependency managmenet using `pip`. 

The `pyproject.toml` file stores all requirements, e.g., having a package with version number no lower than X. The `.venv` folder stores the virtual environment. The `uv.lock` stores the exact packages last used. When you run `uv sync` it checks for a lock file and, if existing, matches that environment exactly. If there is not a lock file, it uses the `pyproject.toml` to install packages. 

Use `uv [add]/[remove] [package]` (e.g., `uv add pandas`) to install or remove packages. You can pin the version numbers as needed. Be sure to commit the `pyproject.toml` and `uv.lock` when changed. 

### R

TBD, but we should probably use `renv` for managing R the environment and packages. 

### Julia 

TBD 

## Data management
### Local files tracked with DVC

Most data are too large to push to Github. We use the [Data Version Control](https://dvc.org/doc) to manage larger files. Follow [these](https://dvc.org/doc/start) instructions for setup (and coordinate with team on what to use for remote storage, so all referencing the same remote files). 

**Adding a file to be tracked**
```bash
# Add DVC file and push it to the remote storage 
dvc add file.csv]
dvc push

# Push the .dvc file that the above step creates so others can use it to pull the data from DVC
git add file.csv.dvc
git commit -m "New DVC-tracked file"
git push 
```
The first step creates a `file.csv.dvc` in the same folder and a `.gitignore` that tells Git to ignore the original data file. 

**Pulling remote file locally**
```bash
# Pull a single file
dvc pull file.csv.dvc

# Pull all .dvc files in a folder 
dvc pull /path/to/data/folder 
```

⚠️ Do not push data files (`.csv`, `.parquet`, `.rds`, ...) to Github. They should instead be tracked with DVC, and only the `.dvc` file should be pushed.  



### Google Cloud Platform (GCP)

We occasionally use GCP tools -- e.g., BigQuery to store large relational databases. Read [here](https://github.com/codyfcook/GentzkowLabTemplate/wiki/Using-Google-Cloud-Platform-(GCP)) for details on how to setup and use various GCP products

These introduce some challenges with tracking data. Instead, track table names in `config.yaml` -- for example, we can add a line to the config that defines the location of Census block group shapefiles as  `bgrp_geos_table: 'communal-data.geos.bgrp_2010'`.  


## Analysis output management 

Output from the analyses (e.g., `.pdf`, `.jpg`, `.png`, `.tex`, etc) can also become too large to just push to Github. While these could be stored with DVC, there's some overhead with DVC that can get annoying for many files. 

Instead, we can use [Git LFS](https://docs.github.com/en/repositories/working-with-files/managing-large-files/about-git-large-file-storage), which uses the exact same workflow as tracking any other file for Github, but stores files with pre-specified suffixes (e.g., `.jpg`) externally. The storage is managed by Github; there's a generous free tier and pricing beyond that is reasonable ([details](https://docs.github.com/en/billing/managing-billing-for-your-products/managing-billing-for-git-large-file-storage/about-billing-for-git-large-file-storage)). 

After [installing](https://docs.github.com/en/repositories/working-with-files/managing-large-files/installing-git-large-file-storage), use commands like `git lfs track "*.jpg"` to set up which files to store with Git LFS. This will create the `.gitattributes` file, which should be pushed to repository. Then the usual git workflow works: `git add example.jpg; git commit -m "message"; git push` 

Note: if the data for a project are reasonably small then Git LFS may be a good option for tracking data, instead of DVC.


# Setup steps (with an example)

The template includes an example that should be run start-to-finish to understand the desired pipeline. The current setup is for MacOS; it may take some work to adapt it to a Windows machine. 

1. Setup a new repository for your project, using this as a template ([details](https://docs.github.com/en/repositories/creating-and-managing-repositories/creating-a-repository-from-a-template)). 

2. Clone the new repository. You can use HTTPS, which requires you to enter username/password, or SSH, which requires you to [generate SSH keys](https://docs.github.com/en/authentication/connecting-to-github-with-ssh/generating-a-new-ssh-key-and-adding-it-to-the-ssh-agent#generating-a-new-ssh-key) and [add them to you Github account](https://docs.github.com/en/authentication/connecting-to-github-with-ssh/adding-a-new-ssh-key-to-your-github-account). Setting up SSH will save you from frequently typing username/password.

3.  Install Git LFS for managing output files and `uv` 
```bash
# Install GitLFS (Optional)
brew install git-lfs           # Mac (Homebrew)
sudo apt-get install git-lfs   # Ubuntu/Debian

# Install uv
curl -LsSf https://astral.sh/uv/install.sh | sh
```

4. Sync the Python environment by running the following from the base directory (i.e. `project_template/`). This will also install DVC to manage data files, because it is specified in the template's `pyproject.toml` 
```bash
uv venv                       # Creates a .venv folder
source .venv/bin/activate     # Activate the virtual environment
uv sync                       # Sync requirements based on uv.lock and/or pyproject.toml 
```

5. (Optional) Setup DVC to manage data files. See [here](https://dvc.org/doc/start) for the basics and [here](https://dvc.org/doc/user-guide/data-management/remote-storage) for connecting to a remote storage like Google Drive or S3 (highly recommended!). 

5. Run the example from start to finish 

    - Clean the data: `bash 1_data_prep/run_all.sh` 

        - (Optional) Push the data to DVC storage: `dvc add /path/to/file.parquet; dvc push`. If the data has already been created, the folder will have a `.dvc` file in it, and can be pulled locally with `dvc pull /path/to/file.dvc`. 
    - Run the analyses: `bash 2_analysis/run_all.sh`         
    - Generate the report: `bash 4_notes/example_submodule/make.sh` 
        - (Optional) Push all the `.pdf` and `.tex` files, storing them in Git LFS


# Credit
This template was inspired by the [GentzkowLabTemplate](https://github.com/gentzkow/GentzkowLabTemplate). 
